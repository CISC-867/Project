{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data.VCTK\n",
    "import model.model\n",
    "import model.trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data.VCTK) # prevent cache when making changes\n",
    "dataset = data.VCTK.VCTKDataset(\"VCTK-Corpus-smaller/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 129184 parameters.\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(model.model) # prevent cache when making changes\n",
    "\n",
    "# deallocate if exists\n",
    "try:\n",
    "    del myModel\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "myModel = model.model.SpectrogramModel().to(device)\n",
    "print(f\"Model has {myModel.get_param_count()} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "\n",
    "# add WaveRNN to path\n",
    "sys.path.append(os.path.abspath(\"ForwardTacotron\"))\n",
    "import models.fatchord_version\n",
    "importlib.reload(models.fatchord_version)\n",
    "\n",
    "# deallocate if exists\n",
    "try:\n",
    "    del vocoderModel\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "vocoderModel = models.fatchord_version.WaveRNN(\n",
    "    rnn_dims=512,\n",
    "    fc_dims=512,\n",
    "    bits=9, # OrigAuthor: bit depth of signal\n",
    "    pad=2, # OrigAuthor: this will pad the input so that the resnet can 'see' wider than input length\n",
    "    upsample_factors=(5, 5, 8), # OrigAuthor: NB - this needs to correctly factorise hop_length\n",
    "    feat_dims=80,\n",
    "    compute_dims=128,\n",
    "    res_out_dims=128,\n",
    "    res_blocks=10,\n",
    "    hop_length=200,\n",
    "    sample_rate=16000,\n",
    "    mode=\"RAW\", # OrigAuthor: either 'RAW' (softmax on raw bits) or 'MOL' (sample from mixture of logistics)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11996.954624 0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "def reasonable(n):\n",
    "    return n/(1000**2)\n",
    "print(reasonable(t),reasonable(r),reasonable(a),reasonable(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clips torch.Size([1, 25600])\n",
      "spectros torch.Size([1, 80, 129])\n",
      "Took 0.0MB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del x\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "before = torch.cuda.memory_allocated(0)\n",
    "\n",
    "text, clips, spectros = dataset[0]\n",
    "batch = 1\n",
    "# take one audio clip and sectrogram, repeat for overfitting test\n",
    "clips = clips[0,:].to(device).repeat(batch,1)\n",
    "spectros = spectros[0,:,:].to(device).repeat(batch,1,1)\n",
    "print(\"clips\", clips.shape)\n",
    "print(\"spectros\", spectros.shape)\n",
    "\n",
    "after = torch.cuda.memory_allocated(0)\n",
    "print(f\"Took {reasonable(after-before):.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 129])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 128])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spectros.shape)\n",
    "myModel(spectros).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del myTrainer\n",
    "except NameError:\n",
    "    pass\n",
    "importlib.reload(model.trainer) # prevent cache when making changes\n",
    "myTrainer = model.trainer.Trainer(myModel, vocoderModel, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(3.6780, grad_fn=<AddBackward0>) tensor(1.5073, grad_fn=<L1LossBackward0>) tensor(2.1707, grad_fn=<DivBackward1>)\n",
      "loss tensor(3.6780, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = myTrainer.train_step(clips, spectros)\n",
    "print(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fadf5cc01743809e51dee0d4d80e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(1.7347, grad_fn=<AddBackward0>) tensor(1.5190, grad_fn=<L1LossBackward0>) tensor(0.2156, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-5.2986, grad_fn=<AddBackward0>) tensor(1.6274, grad_fn=<L1LossBackward0>) tensor(-6.9260, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-29.6101, grad_fn=<AddBackward0>) tensor(1.7403, grad_fn=<L1LossBackward0>) tensor(-31.3504, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-45.4433, grad_fn=<AddBackward0>) tensor(1.9017, grad_fn=<L1LossBackward0>) tensor(-47.3450, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-118.6740, grad_fn=<AddBackward0>) tensor(2.0889, grad_fn=<L1LossBackward0>) tensor(-120.7629, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-285.1589, grad_fn=<AddBackward0>) tensor(2.1406, grad_fn=<L1LossBackward0>) tensor(-287.2996, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-461.2626, grad_fn=<AddBackward0>) tensor(2.3451, grad_fn=<L1LossBackward0>) tensor(-463.6077, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-269.0549, grad_fn=<AddBackward0>) tensor(2.3430, grad_fn=<L1LossBackward0>) tensor(-271.3980, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-1880.3657, grad_fn=<AddBackward0>) tensor(2.3276, grad_fn=<L1LossBackward0>) tensor(-1882.6934, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-1164.4752, grad_fn=<AddBackward0>) tensor(2.3495, grad_fn=<L1LossBackward0>) tensor(-1166.8247, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-4045.0916, grad_fn=<AddBackward0>) tensor(2.3609, grad_fn=<L1LossBackward0>) tensor(-4047.4524, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-4046.8682, grad_fn=<AddBackward0>) tensor(2.4874, grad_fn=<L1LossBackward0>) tensor(-4049.3557, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-474.3238, grad_fn=<AddBackward0>) tensor(2.5447, grad_fn=<L1LossBackward0>) tensor(-476.8685, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-942.8016, grad_fn=<AddBackward0>) tensor(2.5026, grad_fn=<L1LossBackward0>) tensor(-945.3043, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-3523.0857, grad_fn=<AddBackward0>) tensor(2.6134, grad_fn=<L1LossBackward0>) tensor(-3525.6990, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-7571.1025, grad_fn=<AddBackward0>) tensor(2.6633, grad_fn=<L1LossBackward0>) tensor(-7573.7661, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-13812.1113, grad_fn=<AddBackward0>) tensor(2.7088, grad_fn=<L1LossBackward0>) tensor(-13814.8203, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-11188.8291, grad_fn=<AddBackward0>) tensor(2.7809, grad_fn=<L1LossBackward0>) tensor(-11191.6104, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-9924.0107, grad_fn=<AddBackward0>) tensor(2.7855, grad_fn=<L1LossBackward0>) tensor(-9926.7959, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-10691.5029, grad_fn=<AddBackward0>) tensor(2.8698, grad_fn=<L1LossBackward0>) tensor(-10694.3730, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-15790.8877, grad_fn=<AddBackward0>) tensor(2.9020, grad_fn=<L1LossBackward0>) tensor(-15793.7900, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-31314.2852, grad_fn=<AddBackward0>) tensor(2.9862, grad_fn=<L1LossBackward0>) tensor(-31317.2715, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-51451.5898, grad_fn=<AddBackward0>) tensor(2.9389, grad_fn=<L1LossBackward0>) tensor(-51454.5273, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-63777.7773, grad_fn=<AddBackward0>) tensor(2.9877, grad_fn=<L1LossBackward0>) tensor(-63780.7656, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-42853.8203, grad_fn=<AddBackward0>) tensor(3.0906, grad_fn=<L1LossBackward0>) tensor(-42856.9102, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-71709.4219, grad_fn=<AddBackward0>) tensor(3.1068, grad_fn=<L1LossBackward0>) tensor(-71712.5312, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-42037.2383, grad_fn=<AddBackward0>) tensor(3.1190, grad_fn=<L1LossBackward0>) tensor(-42040.3555, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-95471.6406, grad_fn=<AddBackward0>) tensor(3.1105, grad_fn=<L1LossBackward0>) tensor(-95474.7500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-153280.5938, grad_fn=<AddBackward0>) tensor(3.0315, grad_fn=<L1LossBackward0>) tensor(-153283.6250, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-204907.7656, grad_fn=<AddBackward0>) tensor(2.7274, grad_fn=<L1LossBackward0>) tensor(-204910.5000, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-416685.0312, grad_fn=<AddBackward0>) tensor(2.8669, grad_fn=<L1LossBackward0>) tensor(-416687.9062, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-224702.5625, grad_fn=<AddBackward0>) tensor(2.8672, grad_fn=<L1LossBackward0>) tensor(-224705.4375, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-676511.8750, grad_fn=<AddBackward0>) tensor(2.7998, grad_fn=<L1LossBackward0>) tensor(-676514.6875, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-641315.6875, grad_fn=<AddBackward0>) tensor(2.7262, grad_fn=<L1LossBackward0>) tensor(-641318.4375, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-1057966.6250, grad_fn=<AddBackward0>) tensor(2.6604, grad_fn=<L1LossBackward0>) tensor(-1057969.2500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-1641325.1250, grad_fn=<AddBackward0>) tensor(2.7003, grad_fn=<L1LossBackward0>) tensor(-1641327.8750, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-1300748.8750, grad_fn=<AddBackward0>) tensor(2.6932, grad_fn=<L1LossBackward0>) tensor(-1300751.6250, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-1249609.2500, grad_fn=<AddBackward0>) tensor(2.7767, grad_fn=<L1LossBackward0>) tensor(-1249612., grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-1996139.6250, grad_fn=<AddBackward0>) tensor(2.9366, grad_fn=<L1LossBackward0>) tensor(-1996142.5000, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-2827990.2500, grad_fn=<AddBackward0>) tensor(2.9673, grad_fn=<L1LossBackward0>) tensor(-2827993.2500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-3679236.7500, grad_fn=<AddBackward0>) tensor(2.9692, grad_fn=<L1LossBackward0>) tensor(-3679239.7500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-3760093.7500, grad_fn=<AddBackward0>) tensor(2.9748, grad_fn=<L1LossBackward0>) tensor(-3760096.7500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-4214499.5000, grad_fn=<AddBackward0>) tensor(3.0960, grad_fn=<L1LossBackward0>) tensor(-4214502.5000, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-5204633.5000, grad_fn=<AddBackward0>) tensor(3.0603, grad_fn=<L1LossBackward0>) tensor(-5204636.5000, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-5893173., grad_fn=<AddBackward0>) tensor(2.9789, grad_fn=<L1LossBackward0>) tensor(-5893176., grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-7513408.5000, grad_fn=<AddBackward0>) tensor(3.0034, grad_fn=<L1LossBackward0>) tensor(-7513411.5000, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-8224274., grad_fn=<AddBackward0>) tensor(3.0612, grad_fn=<L1LossBackward0>) tensor(-8224277., grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-8794132., grad_fn=<AddBackward0>) tensor(3.0592, grad_fn=<L1LossBackward0>) tensor(-8794135., grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-7281889., grad_fn=<AddBackward0>) tensor(3.1635, grad_fn=<L1LossBackward0>) tensor(-7281892., grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-9092106., grad_fn=<AddBackward0>) tensor(3.2005, grad_fn=<L1LossBackward0>) tensor(-9092109., grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-6896539., grad_fn=<AddBackward0>) tensor(3.0486, grad_fn=<L1LossBackward0>) tensor(-6896542., grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-3769642.7500, grad_fn=<AddBackward0>) tensor(3.0948, grad_fn=<L1LossBackward0>) tensor(-3769645.7500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-3747561., grad_fn=<AddBackward0>) tensor(3.0334, grad_fn=<L1LossBackward0>) tensor(-3747564., grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-243218.0469, grad_fn=<AddBackward0>) tensor(2.9053, grad_fn=<L1LossBackward0>) tensor(-243220.9531, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-127208.8047, grad_fn=<AddBackward0>) tensor(2.7884, grad_fn=<L1LossBackward0>) tensor(-127211.5938, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-141187.5469, grad_fn=<AddBackward0>) tensor(2.7287, grad_fn=<L1LossBackward0>) tensor(-141190.2812, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-148290.8906, grad_fn=<AddBackward0>) tensor(2.6964, grad_fn=<L1LossBackward0>) tensor(-148293.5938, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-211783.5312, grad_fn=<AddBackward0>) tensor(2.6816, grad_fn=<L1LossBackward0>) tensor(-211786.2188, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-222331.2656, grad_fn=<AddBackward0>) tensor(2.6896, grad_fn=<L1LossBackward0>) tensor(-222333.9531, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-167629., grad_fn=<AddBackward0>) tensor(2.7096, grad_fn=<L1LossBackward0>) tensor(-167631.7031, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-180668.4531, grad_fn=<AddBackward0>) tensor(2.7327, grad_fn=<L1LossBackward0>) tensor(-180671.1875, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-269496.5938, grad_fn=<AddBackward0>) tensor(2.7663, grad_fn=<L1LossBackward0>) tensor(-269499.3750, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-243332.4219, grad_fn=<AddBackward0>) tensor(2.8045, grad_fn=<L1LossBackward0>) tensor(-243335.2188, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-256010.3750, grad_fn=<AddBackward0>) tensor(2.8446, grad_fn=<L1LossBackward0>) tensor(-256013.2188, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-347242.8125, grad_fn=<AddBackward0>) tensor(2.8833, grad_fn=<L1LossBackward0>) tensor(-347245.6875, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-279882.0938, grad_fn=<AddBackward0>) tensor(2.9173, grad_fn=<L1LossBackward0>) tensor(-279885., grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-301969.6250, grad_fn=<AddBackward0>) tensor(2.9464, grad_fn=<L1LossBackward0>) tensor(-301972.5625, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-390122.2812, grad_fn=<AddBackward0>) tensor(2.9740, grad_fn=<L1LossBackward0>) tensor(-390125.2500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-435070.3750, grad_fn=<AddBackward0>) tensor(3.0021, grad_fn=<L1LossBackward0>) tensor(-435073.3750, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-477413.7188, grad_fn=<AddBackward0>) tensor(3.0278, grad_fn=<L1LossBackward0>) tensor(-477416.7500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-528542.3125, grad_fn=<AddBackward0>) tensor(3.0555, grad_fn=<L1LossBackward0>) tensor(-528545.3750, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-584262.5625, grad_fn=<AddBackward0>) tensor(3.0802, grad_fn=<L1LossBackward0>) tensor(-584265.6250, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-624791.6250, grad_fn=<AddBackward0>) tensor(3.1027, grad_fn=<L1LossBackward0>) tensor(-624794.7500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-577026.3750, grad_fn=<AddBackward0>) tensor(3.1186, grad_fn=<L1LossBackward0>) tensor(-577029.5000, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-721915., grad_fn=<AddBackward0>) tensor(3.1306, grad_fn=<L1LossBackward0>) tensor(-721918.1250, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-657426.9375, grad_fn=<AddBackward0>) tensor(3.1455, grad_fn=<L1LossBackward0>) tensor(-657430.0625, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-675745.1875, grad_fn=<AddBackward0>) tensor(3.1563, grad_fn=<L1LossBackward0>) tensor(-675748.3750, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(126005.4766, grad_fn=<AddBackward0>) tensor(3.1943, grad_fn=<L1LossBackward0>) tensor(126002.2812, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(68558.3125, grad_fn=<AddBackward0>) tensor(3.2124, grad_fn=<L1LossBackward0>) tensor(68555.1016, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-444368.8125, grad_fn=<AddBackward0>) tensor(3.2454, grad_fn=<L1LossBackward0>) tensor(-444372.0625, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-660697.5000, grad_fn=<AddBackward0>) tensor(3.2337, grad_fn=<L1LossBackward0>) tensor(-660700.7500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-518596.3125, grad_fn=<AddBackward0>) tensor(3.2450, grad_fn=<L1LossBackward0>) tensor(-518599.5625, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-402912.7812, grad_fn=<AddBackward0>) tensor(3.2716, grad_fn=<L1LossBackward0>) tensor(-402916.0625, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-231192.7500, grad_fn=<AddBackward0>) tensor(3.2832, grad_fn=<L1LossBackward0>) tensor(-231196.0312, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-799901.7500, grad_fn=<AddBackward0>) tensor(3.2736, grad_fn=<L1LossBackward0>) tensor(-799905., grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-609572., grad_fn=<AddBackward0>) tensor(3.2752, grad_fn=<L1LossBackward0>) tensor(-609575.2500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-855899.1250, grad_fn=<AddBackward0>) tensor(3.2906, grad_fn=<L1LossBackward0>) tensor(-855902.4375, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-639261.1875, grad_fn=<AddBackward0>) tensor(3.3254, grad_fn=<L1LossBackward0>) tensor(-639264.5000, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-533887.3750, grad_fn=<AddBackward0>) tensor(3.3538, grad_fn=<L1LossBackward0>) tensor(-533890.7500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-633913.3125, grad_fn=<AddBackward0>) tensor(3.3780, grad_fn=<L1LossBackward0>) tensor(-633916.6875, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-616128.1250, grad_fn=<AddBackward0>) tensor(3.3964, grad_fn=<L1LossBackward0>) tensor(-616131.5000, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-154679.7344, grad_fn=<AddBackward0>) tensor(3.3736, grad_fn=<L1LossBackward0>) tensor(-154683.1094, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-848006.6250, grad_fn=<AddBackward0>) tensor(3.4349, grad_fn=<L1LossBackward0>) tensor(-848010.0625, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-856421.5625, grad_fn=<AddBackward0>) tensor(3.4548, grad_fn=<L1LossBackward0>) tensor(-856425., grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-995843.5625, grad_fn=<AddBackward0>) tensor(3.4574, grad_fn=<L1LossBackward0>) tensor(-995847., grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-1079781., grad_fn=<AddBackward0>) tensor(3.4648, grad_fn=<L1LossBackward0>) tensor(-1079784.5000, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-1127254., grad_fn=<AddBackward0>) tensor(3.4647, grad_fn=<L1LossBackward0>) tensor(-1127257.5000, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-1366426.2500, grad_fn=<AddBackward0>) tensor(3.4732, grad_fn=<L1LossBackward0>) tensor(-1366429.7500, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-854318.8125, grad_fn=<AddBackward0>) tensor(3.4812, grad_fn=<L1LossBackward0>) tensor(-854322.3125, grad_fn=<DivBackward1>)\n",
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-1011760.6250, grad_fn=<AddBackward0>) tensor(3.4886, grad_fn=<L1LossBackward0>) tensor(-1011764.1250, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "for i, _ in tqdm.tqdm(list(enumerate(range(0,10)))):\n",
    "    total_loss, spectro_loss, vocoder_loss = \\\n",
    "        myTrainer.train_step(clips, spectros)\n",
    "    print(\\\n",
    "        \"total\", total_loss.numpy(),\\\n",
    "        \"spectro\", spectro_loss.numpy(),\\\n",
    "        \"vocoder\" vocoder_loss.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Won't overwrite existing models",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-b752fc1dd1f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"model2.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Won't overwrite existing models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Won't overwrite existing models"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os.path\n",
    "checkpoint_dir = pathlib.Path(\"checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "model_path = checkpoint_dir / \"model2.pth\"\n",
    "if os.path.exists(model_path):\n",
    "    raise Exception(\"Won't overwrite existing models\")\n",
    "else:\n",
    "    torch.save(myModel.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8505bd93e15232b680218bd613f68dd2d0ec76b40a79f48f6cb2b19121cd32c4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('azureml_py36_pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
