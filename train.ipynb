{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data.VCTK\n",
    "import model.model\n",
    "import model.trainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data.VCTK) # prevent cache when making changes\n",
    "dataset = data.VCTK.VCTKDataset(\"VCTK-Corpus-smaller/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 129184 parameters.\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(model.model) # prevent cache when making changes\n",
    "\n",
    "# deallocate if exists\n",
    "try:\n",
    "    del myModel\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "myModel = model.model.SpectrogramModel().to(device)\n",
    "print(f\"Model has {myModel.get_param_count()} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os.path\n",
    "\n",
    "# add WaveRNN to path\n",
    "sys.path.append(os.path.abspath(\"ForwardTacotron\"))\n",
    "import models.fatchord_version\n",
    "importlib.reload(models.fatchord_version)\n",
    "\n",
    "# deallocate if exists\n",
    "try:\n",
    "    del vocoderModel\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "vocoderModel = models.fatchord_version.WaveRNN(\n",
    "    rnn_dims=512,\n",
    "    fc_dims=512,\n",
    "    bits=9, # OrigAuthor: bit depth of signal\n",
    "    pad=2, # OrigAuthor: this will pad the input so that the resnet can 'see' wider than input length\n",
    "    upsample_factors=(5, 5, 8), # OrigAuthor: NB - this needs to correctly factorise hop_length\n",
    "    feat_dims=80,\n",
    "    compute_dims=128,\n",
    "    res_out_dims=128,\n",
    "    res_blocks=10,\n",
    "    hop_length=200,\n",
    "    sample_rate=16000,\n",
    "    mode=\"RAW\", # OrigAuthor: either 'RAW' (softmax on raw bits) or 'MOL' (sample from mixture of logistics)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11996.954624 0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "t = torch.cuda.get_device_properties(0).total_memory\n",
    "r = torch.cuda.memory_reserved(0)\n",
    "a = torch.cuda.memory_allocated(0)\n",
    "f = r-a  # free inside reserved\n",
    "def reasonable(n):\n",
    "    return n/(1000**2)\n",
    "print(reasonable(t),reasonable(r),reasonable(a),reasonable(f))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clips torch.Size([1, 25600])\n",
      "spectros torch.Size([1, 80, 129])\n",
      "Took 0.0MB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del x\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "before = torch.cuda.memory_allocated(0)\n",
    "\n",
    "text, clips, spectros = dataset[0]\n",
    "batch = 1\n",
    "# take one audio clip and sectrogram, repeat for overfitting test\n",
    "clips = clips[0,:].to(device).repeat(batch,1)\n",
    "spectros = spectros[0,:,:].to(device).repeat(batch,1,1)\n",
    "print(\"clips\", clips.shape)\n",
    "print(\"spectros\", spectros.shape)\n",
    "\n",
    "after = torch.cuda.memory_allocated(0)\n",
    "print(f\"Took {reasonable(after-before):.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 129])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 128])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spectros.shape)\n",
    "myModel(spectros).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del myTrainer\n",
    "except NameError:\n",
    "    pass\n",
    "importlib.reload(model.trainer) # prevent cache when making changes\n",
    "myTrainer = model.trainer.Trainer(myModel, vocoderModel, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(3.6780, grad_fn=<AddBackward0>) tensor(1.5073, grad_fn=<L1LossBackward0>) tensor(2.1707, grad_fn=<DivBackward1>)\n",
      "loss tensor(3.6780, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = myTrainer.train_step(clips, spectros)\n",
    "print(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990c0a82571e4ce088abb33c3d569488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teehee\n",
      "audios torch.Size([1, 25600]) pred torch.Size([1, 25600])\n",
      "tensor(-939794.7500, grad_fn=<AddBackward0>) tensor(3.4942, grad_fn=<L1LossBackward0>) tensor(-939798.2500, grad_fn=<DivBackward1>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-de6626531d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspectro_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocoder_loss\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mmyTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclips\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspectros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     print(\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m\"total\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;31m# See gh-54457\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration over a 0-d tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "for i, _ in tqdm.tqdm(list(enumerate(range(0,10)))):\n",
    "    total_loss, spectro_loss, vocoder_loss = \\\n",
    "        myTrainer.train_step(clips, spectros)\n",
    "    print(\n",
    "        \"total\", total_loss.numpy(),\n",
    "        \"spectro\", spectro_loss.numpy(),\n",
    "        \"vocoder\", vocoder_loss.numpy()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Won't overwrite existing models",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-b752fc1dd1f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"model2.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Won't overwrite existing models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Won't overwrite existing models"
     ]
    }
   ],
   "source": [
    "myModel.save_as(\"model3.pth\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8505bd93e15232b680218bd613f68dd2d0ec76b40a79f48f6cb2b19121cd32c4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('azureml_py36_pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
